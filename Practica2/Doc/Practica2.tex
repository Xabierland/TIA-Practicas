\documentclass{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, longtable, float, titlesec, hyperref, enumitem, dingbat, soul, multicol, listings}
\usepackage[dvipsnames]{xcolor}
\usepackage[margin=2cm]{geometry}

% Cambia el color de los links
\hypersetup{
    hidelinks = true
}

% Python Code
\lstdefinestyle{Python}{
  commentstyle=\color{brown},
  keywordstyle=\color{violet},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{purple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2,
  literate={ñ}{{\~n}}1 {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
}
\lstset{style=Python}

% Elimina la palabra "Capítulo" de los títulos de los capítulos
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge\thechapter.\space}

\titleformat{name=\chapter,numberless}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\titlespacing*{\chapter}{0pt}{-50pt}{20pt}

% Personalización del índice de listados
\renewcommand{\lstlistingname}{Código}  % Cambiar el nombre de "Listing" a "Código"
\renewcommand{\lstlistlistingname}{Índice de Códigos}

% Añade numeración a los subsubsection*s y los añade al índice
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\begin{document}
    \begin{titlepage}
        \centering
        \includegraphics[width=0.6\textwidth]{./.img/logo.jpg}\\
        \vspace{1cm}
        \LARGE Técnicas de Inteligencia Artificial\\
        \vspace{0.5cm}
        \Large Ingeniería Informática de Gestión y Sistemas de Información\\
        \vspace{3cm}
        \Huge Practica 2\\
        \huge Búsqueda Multi-Agente\\
        \vspace{2.5cm}
        \Large Autor(es):\\
        \vspace{0.2cm}
        \large Xabier Gabiña\\
        \large Diego Montoya\\
        \vfill
        \today
    \end{titlepage}
    \tableofcontents
    \listoffigures
    \listoftables
    \lstlistoflistings
    \chapter{Introducción}
      \paragraph*{}{
        En esta práctica se implementarán varios algoritmos de búsqueda multi-agente para el juego de Pacman. 
        Los algoritmos a implementar son los siguientes:
        \begin{itemize}
          \item Agente Reflex: Un agente que selecciona acciones a corto plazo basándose en la percepción actual del entorno.
          \item Minimax: Un algoritmo de búsqueda en árboles que se utiliza en juegos de dos o más jugadores y deterministas.
          \item Podado Alpha-Beta: Una mejora del algoritmo Minimax que reduce el número de nodos evaluados en el árbol de búsqueda.
          \item Expectimax: Una variante del algoritmo Minimax que se utiliza cuando los oponentes no toman siempre la mejor decisión posible.
        \end{itemize}
        Además, se mejorará la función de evaluación del agente reflex para que tome en cuenta más factores del estado del juego.
      }
    \chapter{Ejercicios}
      \section{Agente Reflex}
        \subsection*{Descripción}
          \paragraph*{}{
            Un agente reflex es un agente que selecciona acciones a corto plazo basándose en la percepción actual del entorno. 
            En este caso, el agente reflex se basa en una función de evaluación para seleccionar la mejor acción en cada estado. 
            La función de evaluación asigna un valor numérico a cada estado, y el agente selecciona la acción que maximiza este valor.
            Para asignar un valor a cada estado, la función de evaluación considera varios factores, como la distancia a la comida, la proximidad a los fantasmas y la cantidad de comida restante.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{./.img/reflex.png}
                \caption{Ejemplo de agente reflex}
            \end{figure}  
        }
        \subsection*{Primera implementación}
          \begin{lstlisting}[language=Python, caption=Implementación inicial del agente reflex]
class ReflexAgent(Agent):
    """
    A reflex agent chooses an action at each choice point by examining
    its alternatives via a state evaluation function.

    The code below is provided as a guide.  You are welcome to change
    it in any way you see fit, so long as you don't touch our method
    headers.
    """

    def getAction(self, gameState):
        """
        You do not need to change this method, but you're welcome to.

        getAction chooses among the best options according to the evaluation function.

        Just like in the previous project, getAction takes a GameState and returns
        some Directions.X for some X in the set {NORTH, SOUTH, WEST, EAST, STOP}
        """
        # Collect legal moves and successor states
        legalMoves = gameState.getLegalActions()

        # Choose one of the best actions
        scores = [self.evaluationFunction(gameState, action) for action in legalMoves]
        bestScore = max(scores)
        bestIndices = [index for index in range(len(scores)) if scores[index] == bestScore]
        chosenIndex = random.choice(bestIndices)  # Pick randomly among the best

        return legalMoves[chosenIndex]

    def evaluationFunction(self, currentGameState, action):
        """
        Función de evaluación para Pacman que considera la distancia a la comida, la proximidad a los fantasmas y la cantidad de comida restante.
        """

        # Useful information you can extract from a GameState (pacman.py)
        successorGameState = currentGameState.generatePacmanSuccessor(action)
        newPos = successorGameState.getPacmanPosition()
        newFood = successorGameState.getFood()
        newGhostStates = successorGameState.getGhostStates()
        newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]

        score = successorGameState.getScore()

        # 1. Distancia a la comida más cercana
        foodList = newFood.asList()
        if foodList:
            minFoodDistance = min([manhattanDistance(newPos, food) for food in foodList])
            score += 10.0 / minFoodDistance

        # 2. Distancia a los fantasmas
        for ghost in newGhostStates:
            ghostPos = ghost.getPosition()
            ghostDistance = manhattanDistance(newPos, ghostPos)
            if ghostDistance < 2 and ghost.scaredTimer == 0:
                score -= 1000

        # 3. Penalización por comida restante
        score -= len(foodList) * 10

        return score
          \end{lstlisting}
        \subsection*{Implementación final}
          \begin{lstlisting}[language=Python, caption=Implementación final del agente reflex]
class ReflexAgent(Agent):
    """
    Un agente reflexivo que elige acciones basándose en una función de evaluación.
    """

    def getAction(self, gameState):
        """
        Devuelve la mejor acción para Pacman basada en la función de evaluación.
        """
        # Obtiene las acciones legales para Pacman
        legalMoves = gameState.getLegalActions()

        # Calcula los puntajes para cada acción utilizando la función de evaluación
        scores = [self.evaluationFunction(gameState, action) for action in legalMoves]
        bestScore = max(scores)  # Encuentra el puntaje más alto
        bestIndices = [index for index in range(len(scores)) if scores[index] == bestScore]
        chosenIndex = random.choice(bestIndices)  # Elige aleatoriamente entre las mejores acciones

        return legalMoves[chosenIndex]

    def evaluationFunction(self, currentGameState, action):
        """
        Calcula el valor del estado sucesor después de que Pacman toma la acción `action`.
        Devuelve un valor numérico mayor para estados más favorables.
        """
        # Generar el estado sucesor
        successorGameState = currentGameState.generatePacmanSuccessor(action)
        # Obtiene la posición de Pacman después de moverse
        newPos = successorGameState.getPacmanPosition()
        # Obtiene la matriz de comida en el estado sucesor
        newFood = successorGameState.getFood()
        # Obtiene la lista de estados de los fantasmas
        newGhostStates = successorGameState.getGhostStates()
        # Obtiene los tiempos restantes de los fantasmas asustados
        newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]

        # Inicializar el puntaje con el puntaje base del sucesor
        score = successorGameState.getScore()

        # 1. Distancia a la comida más cercana
        foodList = newFood.asList()  # Convertir la matriz de comida a una lista de posiciones
        if foodList:  # Si hay comida disponible
            # Calcular la distancia mínima a la comida más cercana
            minFoodDistance = min([manhattanDistance(newPos, food) for food in foodList])
            # Invertir la distancia para que un menor valor de distancia dé un mayor puntaje
            score += 10.0 / minFoodDistance

        # 2. Distancia a los fantasmas no asustados
        for ghost in newGhostStates:
            ghostPos = ghost.getPosition()
            ghostDistance = manhattanDistance(newPos, ghostPos)
            if ghostDistance < 2 and ghost.scaredTimer == 0:  # Fantasma no asustado y muy cerca
                score -= 1000  # Penalización fuerte por estar demasiado cerca de un fantasma peligroso

        # 3. Incentivo por acercarse a fantasmas asustados
        for i, ghost in enumerate(newGhostStates):
            if newScaredTimes[i] > 0:  # Si el fantasma está asustado
                ghostDistance = manhattanDistance(newPos, ghost.getPosition())
                score += 200.0 / (ghostDistance + 1)  # Premiar estar cerca de un fantasma asustado

        # 4. Penalización por comida restante
        score -= len(foodList) * 10  # Penalizar por cada comida restante en el estado sucesor

        # 5. Penalización por cápsulas de poder restantes
        capsules = successorGameState.getCapsules()
        score -= len(capsules) * 100

        return score
          \end{lstlisting}
        \subsection*{Comentarios}
          \paragraph*{}{
            La implementación de este ejercicio es bastante sencilla aunque rara.
            En nuestra primera implementación hemos obviado el tema de los pelets y por ende el de los fantasmas asustados.
            No obstante la puntuacion no ha sido mala.
            Aun asi cuando hemos hecho la implementación final añadiendo ambas metricas la realidad es que la puntuación se ha mantenido igual.
            Creemos que es debido a los sencillos mapas a los que se somete al agente y a que la función de evaluación ya era bastante buena para ellos.
        }
      \clearpage\section{Minimax} %TERMINADO
        \subsection*{Descripción}
          \paragraph*{}{
            Minimax es un algoritmo de búsqueda en árboles que se utiliza en juegos de dos o más jugadores y deterministas.
            El algoritmo Minimax se basa en la idea de que los jugadores intentan maximizar su puntaje, mientras que sus oponentes intentan minimizarlo, es decir, juegos de suma cero.
            Este algoritmo es especialmente útil cuando el oponente toma siempre la mejor decisión posible.

            El algoritmo Minimax funciona de la siguiente manera:
            \begin{enumerate}
                \item Se construye un árbol de busqueda con todos los posibles movimientos.
                \item Cada nodo del árbol representa un estado del juego y cada rama representa una acción.
                \item El arbol alterna entre los jugadores, uno intenta maximizar el puntaje y los otros minimizarlo.
                \item Se evaluan los nodos terminales del árbol y se asigna un valor usando una función de evaluación.
                \item Dependiendo del jugador, se selecciona el nodo hijo con el valor máximo o mínimo.
            \end{enumerate}
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{./.img/minimax.png}
                \caption{Ejemplo de árbol de búsqueda Minimax}
            \end{figure}
            
            Respecto al coste tanto en tiempo como en espacio, el algoritmo Minimax es, en esencia un DFS por lo que su coste en tiempo es $O(b^m)$ y en espacio $O(bm)$. 
            Estos malos costes se deben a que el algoritmo explora todo el árbol de búsqueda, lo que lo hace inviable en juegos con un gran factor de ramificación y profundidad.
          }
        \subsection*{Primera implementación}
          \begin{lstlisting}[language=Python, caption=Implementación inicial del agente Minimax]
class MinimaxAgent(MultiAgentSearchAgent):
  """
  Agente que implementa el algoritmo Minimax.
  """

  def getAction(self, gameState):
      """
      Devuelve la mejor acción para Pacman desde el estado actual `gameState` usando Minimax.
      """
      # Llama a la función minimax empezando con el agente 0 (Pacman) y profundidad 0
      best_action, _ = self.minimax(gameState, agentIndex=0, depth=0)
      return best_action

  def minimax(self, gameState, agentIndex, depth):
      """
      Función minimax que devuelve la mejor acción y su valor para el agente actual.
      """
      # Si el estado es terminal (gana o pierde) o alcanzamos la profundidad máxima, evaluamos el estado
      if gameState.isWin() or gameState.isLose() or depth == self.depth:
          return None, self.evaluationFunction(gameState)

      if agentIndex == 0: # Pacman - Maximizador
          return self.max_value(gameState, agentIndex, depth)
      else:               # Fantasmas - Minimizadores
          return self.min_value(gameState, agentIndex, depth)

  def max_value(self, gameState, agentIndex, depth):
      """
      Calcula el valor máximo para el agente Pacman (maximizador).
      """
      # Inicializar el mejor valor y la mejor acción
      best_value = float('-inf')
      best_action = None

      # Recorre todas las acciones legales para Pacman
      for action in gameState.getLegalActions(agentIndex):
          # Generar el estado sucesor
          successorState = gameState.generateSuccessor(agentIndex, action)
          # Calcular el valor del sucesor usando minimax con el siguiente agente
          _, successor_value = self.minimax(successorState, 1, depth)
          # Actualiza el valor máximo si se encuentra un mejor valor
          if successor_value > best_value:
              best_value = successor_value
              best_action = action

      return best_action, best_value

  def min_value(self, gameState, agentIndex, depth):
      """
      Calcula el valor mínimo para los fantasmas (minimizador).
      """
      # Inicializar el peor valor y la mejor acción
      worst_value = float('inf')
      best_action = None

      # Recorre todas las acciones legales para el fantasma actual
      for action in gameState.getLegalActions(agentIndex):
          # Generar el estado sucesor
          successorState = gameState.generateSuccessor(agentIndex, action)
          # Calcula el valor del sucesor con Pacman y siguiente nivel de profundidad
          _, successor_value = self.minimax(successorState, 0, depth + 1)

          # Actualiza el valor mínimo si se encuentra un peor valor
          if successor_value < worst_value:
              worst_value = successor_value
              best_action = action

      return best_action, worst_value

          \end{lstlisting}
        \subsection*{Implementación final}
          \begin{lstlisting}[language=Python, caption=Implementación final del agente Minimax]
class MinimaxAgent(MultiAgentSearchAgent):
  """
  Agente que implementa el algoritmo Minimax.
  """

  def getAction(self, gameState):
      """
      Devuelve la mejor acción para Pacman desde el estado actual `gameState` usando Minimax.
      """
      # Llama a la función minimax empezando con el agente 0 (Pacman) y profundidad 0
      best_action, _ = self.minimax(gameState, agentIndex=0, depth=0)
      return best_action

  def minimax(self, gameState, agentIndex, depth):
      """
      Función minimax que devuelve la mejor acción y su valor para el agente actual.
      """
      # Si el estado es terminal (gana o pierde) o alcanzamos la profundidad máxima, evaluamos el estado
      if gameState.isWin() or gameState.isLose() or depth == self.depth:
          return None, self.evaluationFunction(gameState)

      if agentIndex == 0: # Pacman - Maximizador
          return self.max_value(gameState, agentIndex, depth)
      else:               # Fantasmas - Minimizadores
          return self.min_value(gameState, agentIndex, depth)

  def max_value(self, gameState, agentIndex, depth):
      """
      Calcula el valor máximo para el agente Pacman (maximizador).
      """
      # Inicializar el mejor valor y la mejor acción
      best_value = float('-inf')
      best_action = None

      # Recorre todas las acciones legales para Pacman
      for action in gameState.getLegalActions(agentIndex):
          # Generar el estado sucesor
          successorState = gameState.generateSuccessor(agentIndex, action)
          # Calcular el valor del sucesor usando minimax con el siguiente agente
          _, successor_value = self.minimax(successorState, agentIndex + 1, depth)
          # Actualiza el valor máximo si se encuentra un mejor valor
          if successor_value > best_value:
              best_value = successor_value
              best_action = action

      return best_action, best_value

  def min_value(self, gameState, agentIndex, depth):
      """
      Calcula el valor mínimo para los fantasmas (minimizador).
      """
      # Inicializar el peor valor y la mejor acción
      worst_value = float('inf')
      best_action = None

      # Recorre todas las acciones legales para el fantasma actual
      for action in gameState.getLegalActions(agentIndex):
          # Generar el estado sucesor
          successorState = gameState.generateSuccessor(agentIndex, action)
          
          # Si es el último fantasma, pasamos a Pacman incrementando la profundidad
          if agentIndex == gameState.getNumAgents() - 1:
              # Calcula el valor del sucesor con Pacman y siguiente nivel de profundidad
              _, successor_value = self.minimax(successorState, 0, depth + 1)
          else:
              # Calcula el valor del sucesor con el siguiente fantasma
              _, successor_value = self.minimax(successorState, agentIndex + 1, depth)

          # Actualiza el valor mínimo si se encuentra un peor valor
          if successor_value < worst_value:
              worst_value = successor_value
              best_action = action

      return best_action, worst_value
        
          \end{lstlisting}
        \subsection*{Comentarios}
          \paragraph*{}{
            En la primera implementación no he tenido en cuenta el número de agentes, por lo que el algoritmo solo funcionaba con dos agentes. En la implementación final, he tenido en cuenta el número de agentes y he modificado la función \texttt{min\_value} para que si el agente actual es el último fantasma, pase a Pacman y aumente la profundidad.
          }
      \clearpage\section{Podado Alpha-Beta} %TERMINADO
        \subsection*{Descripción}
          \paragraph*{}
          {
            El algoritmo Alpha-Beta es una mejora del algoritmo Minimax que reduce el número de nodos evaluados en el árbol de búsqueda. 
            Alpha-Beta mantiene dos valores, alpha y beta, que representan los valores de los nodos máximos y mínimos encontrados hasta el momento, respectivamente. 
            El algoritmo poda las ramas del árbol que no afectarán la decisión final, es decir, las ramas que no cambiarán el valor de la raíz del árbol.
            El algoritmo Alpha-Beta es especialmente útil en juegos con muchos nodos y profundidad, ya que reduce el tiempo de búsqueda en el árbol.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{./.img/alpha-beta.jpg}
                \caption{Ejemplo de poda Alpha-Beta}
            \end{figure}

            Respecto al coste tanto en tiempo como en espacio, el algoritmo Alpha-Beta es una mejora del algoritmo Minimax, por lo que su coste en tiempo es $O(b^{m/2})$ y en espacio $O(bm)$.
          }
        \subsection*{Implementación final}
            \begin{lstlisting}[language=Python, caption=Implementación final del agente Alpha-Beta]
class AlphaBetaAgent(MultiAgentSearchAgent):
    """
    Implementacion del minimax con poda alfa-beta.
    """

    def getAction(self, gameState):
        """
        Devuelve la mejor acción para Pacman desde el estado actual `gameState` usando Minimax.
        """
        # Llama a la función minimax empezando con el agente 0 (Pacman) y profundidad 0
        best_action, _ = self.minimax(gameState, agentIndex=0, depth=0, alpha=float('-inf'), beta=float('inf'))
        return best_action

    def minimax(self, gameState, agentIndex, depth, alpha, beta):
        """
        Función minimax que devuelve la mejor acción y su valor para el agente actual.
        """
        # Si el estado es terminal (gana o pierde) o alcanzamos la profundidad máxima, evaluamos el estado
        if gameState.isWin() or gameState.isLose() or depth == self.depth:
            return None, self.evaluationFunction(gameState)

        if agentIndex == 0: # Pacman - Maximizador
            return self.max_value(gameState, agentIndex, depth, alpha, beta)
        else:               # Fantasmas - Minimizadores
            return self.min_value(gameState, agentIndex, depth, alpha, beta)

    def max_value(self, gameState, agentIndex, depth, alpha, beta):
        """
        Calcula el valor máximo para el agente Pacman (maximizador).
        """
        # Inicializar el mejor valor y la mejor acción
        best_value = float('-inf')
        best_action = None

        # Recorre todas las acciones legales para Pacman
        for action in gameState.getLegalActions(agentIndex):
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            # Calcular el valor del sucesor usando minimax con el siguiente agente
            _, successor_value = self.minimax(successorState, agentIndex + 1, depth, alpha, beta)
            # Actualiza el valor máximo si se encuentra un mejor valor
            if successor_value > best_value:
                best_value = successor_value
                best_action = action
                
            if best_value > beta:
                return best_action, best_value
            alpha = max(alpha, best_value)

        return best_action, best_value

    def min_value(self, gameState, agentIndex, depth, alpha, beta):
        """
        Calcula el valor mínimo para los fantasmas (minimizador).
        """
        # Inicializar el peor valor y la mejor acción
        worst_value = float('inf')
        best_action = None

        # Recorre todas las acciones legales para el fantasma actual
        for action in gameState.getLegalActions(agentIndex):
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            
            # Si es el último fantasma, pasamos a Pacman incrementando la profundidad
            if agentIndex == gameState.getNumAgents() - 1:
                # Calcula el valor del sucesor con Pacman y siguiente nivel de profundidad
                _, successor_value = self.minimax(successorState, 0, depth + 1, alpha, beta)
            else:
                # Calcula el valor del sucesor con el siguiente fantasma
                _, successor_value = self.minimax(successorState, agentIndex + 1, depth, alpha, beta)

            # Actualiza el valor mínimo si se encuentra un peor valor
            if successor_value < worst_value:
                worst_value = successor_value
                best_action = action
                
            if worst_value < alpha:
                return best_action, worst_value
            beta = min(beta, worst_value)

        return best_action, worst_value
            \end{lstlisting}
        \subsection*{Comentarios}
          \paragraph*{}{
            En este caso, la implementación es practicamente igual a la del algoritmo Minimax, con la única diferencia de que se han añadido los parámetros alpha y beta a las funciones \texttt{max\_value} y \texttt{min\_value} para realizar la poda junto con las condiciones de corte.
            De esta forma, si el valor de un nodo es mayor que el valor de beta en el caso de un nodo maximizador o menor que alpha en el caso de un nodo minimizador, se poda la rama.
          }
      \clearpage\section{Expectimax}
        \subsection*{Descripción}
          \paragraph*{}{
            El algoritmo Expectimax es una variante del algoritmo Minimax que se utiliza cuando los oponentes no toman siempre la mejor decisión posible.
            En lugar de minimizar el valor de los nodos de los oponentes, el algoritmo Expectimax multiplicará el valor de los nodos de los oponentes por la probabilidad de que tomen una determinada acción.
            En este caso, se asume que las acciones son equiprobables, por lo que la probabilidad de que un oponente tome una acción es 1/n, donde n es el número de acciones legales.
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{./.img/expectimax.png}
                \caption{Ejemplo de árbol de búsqueda Expectimax}
            \end{figure}

            Respecto al coste tanto en tiempo como en espacio, el algoritmo Expectimax es un DFS igual que el Minimax, por lo que su coste en tiempo es $O(b^m)$ y en espacio $O(bm)$.
          }
        \subsection*{Primera implementación}
          \begin{lstlisting}[language=Python, caption=Implementación inicial del agente Expectimax]
class ExpectimaxAgent(MultiAgentSearchAgent):
    """
      Implementacion del algoritmo Expectimax.
    """

    def getAction(self, gameState):
        return self.expectimax(gameState, agentIndex=0, depth=0)
    
    def expectimax(self, gameState, agentIndex, depth):
        """
        Función expectimax que devuelve la mejor acción y su valor para el agente actual.
        """
        # Si el estado es terminal (gana o pierde) o alcanzamos la profundidad máxima, evaluamos el estado
        if gameState.isWin() or gameState.isLose() or depth == self.depth:
            return None, self.evaluationFunction(gameState)

        if agentIndex == 0: # Pacman - Maximizador
            return self.max_value(gameState, agentIndex, depth)
        else:               # Fantasmas - Minimizadores
            return self.exp_value(gameState, agentIndex, depth)
        
    def max_value(self, gameState, agentIndex, depth):
        """
        Calcula el valor máximo para el agente Pacman (maximizador).
        """
        # Inicializar el mejor valor y la mejor acción
        best_value = float('-inf')
        best_action = None

        # Recorre todas las acciones legales para Pacman
        for action in gameState.getLegalActions(agentIndex):
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            # Calcular el valor del sucesor usando expectimax con el siguiente agente
            _, successor_value = self.expectimax(successorState, agentIndex + 1, depth)
            # Actualiza el valor máximo si se encuentra un mejor valor
            if successor_value > best_value:
                best_value = successor_value
                best_action = action

        return best_action, best_value
    
    def exp_value(self, gameState, agentIndex, depth):
        """
        Calcula el valor esperado para los fantasmas (minimizador).
        """
        # Inicializar el valor esperado y la mejor acción
        expected_value = 0
        best_action = None

        # Recorre todas las acciones legales para el fantasma actual
        for action in gameState.getLegalActions(agentIndex):
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            
            # Si es el último fantasma, pasamos a Pacman incrementando la profundidad
            if agentIndex == gameState.getNumAgents() - 1:
                # Calcula el valor del sucesor con Pacman y siguiente nivel de profundidad
                _, successor_value = self.expectimax(successorState, 0, depth + 1)
            else:
                # Calcula el valor del sucesor con el siguiente fantasma
                _, successor_value = self.expectimax(successorState, agentIndex + 1, depth)
            
            # Actualiza el valor esperado con el valor del sucesor
            expected_value += successor_value / len(gameState.getLegalActions(agentIndex))

        return best_action, expected_value
          \end{lstlisting}
        \subsection*{Implementación final}
            \begin{lstlisting}[language=Python, caption=Implementación final del agente Expectimax]
class ExpectimaxAgent(MultiAgentSearchAgent):
    """
        Implementacion del algoritmo Expectimax.
    """

    def getAction(self, gameState):
        best_action, _ = self.expectimax(gameState, agentIndex=0, depth=0)
        return best_action
    
    def expectimax(self, gameState, agentIndex, depth):
        """
        Función expectimax que devuelve la mejor acción y su valor para el agente actual.
        """
        # Si el estado es terminal (gana o pierde) o alcanzamos la profundidad máxima, evaluamos el estado
        if gameState.isWin() or gameState.isLose() or depth == self.depth:
            return None, self.evaluationFunction(gameState)

        if agentIndex == 0: # Pacman - Maximizador
            return self.max_value(gameState, agentIndex, depth)
        else:               # Fantasmas - Minimizadores
            return self.exp_value(gameState, agentIndex, depth)
        
    def max_value(self, gameState, agentIndex, depth):
        """
        Calcula el valor máximo para el agente Pacman (maximizador).
        """
        # Inicializar el mejor valor y la mejor acción
        best_value = float('-inf')
        best_action = None

        # Recorre todas las acciones legales para Pacman
        for action in gameState.getLegalActions(agentIndex):
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            # Calcular el valor del sucesor usando expectimax con el siguiente agente
            _, successor_value = self.expectimax(successorState, agentIndex + 1, depth)
            # Actualiza el valor máximo si se encuentra un mejor valor
            if successor_value > best_value:
                best_value = successor_value
                best_action = action

        return best_action, best_value
    
    def exp_value(self, gameState, agentIndex, depth):
        """
        Calcula el valor esperado para los fantasmas (minimizador).
        """
        # Inicializar el valor esperado y la mejor acción
        expected_value = 0
        best_action = None

        # Recorre todas las acciones legales para el fantasma actual
        actions = gameState.getLegalActions(agentIndex)
        for action in actions:
            # Generar el estado sucesor
            successorState = gameState.generateSuccessor(agentIndex, action)
            
            # Si es el último fantasma, pasamos a Pacman incrementando la profundidad
            if agentIndex == gameState.getNumAgents() - 1:
                # Calcula el valor del sucesor con Pacman y siguiente nivel de profundidad
                _, successor_value = self.expectimax(successorState, 0, depth + 1)
            else:
                # Calcula el valor del sucesor con el siguiente fantasma
                _, successor_value = self.expectimax(successorState, agentIndex + 1, depth)
            # Actualiza el valor esperado con el valor del sucesor
            expected_value += successor_value / len(actions)

        return best_action, expected_value            
            \end{lstlisting}
        \subsection*{Comentarios}
          \paragraph*{}{
            En esta implementación hubo un error anecdotico practicamente en el que al copiar el código de Minimax como base no copie la funcion getAction() correctamente y al implementarla de memoria la hice mal.
            Más haya de ello, la implementación se basa en cambiar la funcion \texttt{min\_value} por \texttt{exp\_value} y en vez de devolver el valor mínimo de los nodos sucesores, se devuelve la media de los valores.
            De esta forma se tiene en cuenta que los oponentes no siempre toman la mejor decisión posible.
          }
      \clearpage\section{Función de Evaluación}
        \subsection*{Descripción}
          \paragraph*{}{
            Este ejercicio, consiste en mejorar la función de evaluación del agente reflex. 
            La función de evaluación asigna un valor numérico a cada estado, y el agente selecciona la acción que maximiza este valor. 
            Para mejorar la implementación previa unicamente se han añadido más factores a tener en cuenta en la evaluación de los estados.
            Para asignar un valor a cada estado, la función de evaluación considera varios factores, como la distancia a la comida, la proximidad a los fantasmas y la cantidad de comida restante.
          }
        \subsection*{Primera implementación}
          \begin{lstlisting}[language=Python, caption=Implementación inicial de la función de evaluación]
def betterEvaluationFunction(currentGameState):
    """
    Función de evaluación para Pacman que considera múltiples factores del estado de juego.
    """

    # Obtener el estado actual de Pacman
    pacmanPos = currentGameState.getPacmanPosition()
    
    # Obtener la comida restante y convertirla a una lista de posiciones
    food = currentGameState.getFood().asList()
    
    # Obtener las posiciones de los fantasmas y sus estados (asustados o no)
    ghostStates = currentGameState.getGhostStates()
    ghostPositions = [ghost.getPosition() for ghost in ghostStates]
    scaredTimes = [ghostState.scaredTimer for ghostState in ghostStates]
    
    # Obtener las pelets de poder restantes
    capsules = currentGameState.getCapsules()
    
    # Obtener el puntaje actual del juego
    score = currentGameState.getScore()

    # 1. Distancia a la comida: Minimizar la distancia a la comida
    if food:
        minFoodDist = min([manhattanDistance(pacmanPos, foodPos) for foodPos in food])
        # Dar un peso inversamente proporcional a la distancia a la comida
        score += 1.0 / (minFoodDist + 1)  # Sumar al score, +1 para evitar división por 0

    # 2. Distancia a los fantasmas: Maximizar la distancia a los fantasmas (si no están asustados)
    for i, ghostPos in enumerate(ghostPositions):
        ghostDist = manhattanDistance(pacmanPos, ghostPos)
        
        if scaredTimes[i] > 0:  # Fantasma asustado
            # Acercarse a los fantasmas asustados para ganar puntos al comérselos
            score += 10.0 / (ghostDist + 1)  # Dar un peso a la proximidad a fantasmas asustados
        else:  # Fantasma no asustado
            # Alejarse de los fantasmas si están demasiado cerca
            if ghostDist > 0:
                score -= 10.0 / ghostDist  # Penalizar por estar cerca de un fantasma peligroso

    # 3. Comida restante: Cuanta menos comida quede, mejor es el estado
    score -= 4.0 * len(food)  # Penalizar más comida restante

    # 4. Cápsulas de poder: Incentivar estar cerca de las pelets de poder
    if capsules:
        minCapsuleDist = min([manhattanDistance(pacmanPos, capsule) for capsule in capsules])
        score += 5.0 / (minCapsuleDist + 1)  # Dar un peso inverso a la distancia a las pelets
        score -= 10 * len(capsules)  # Penalizar si quedan muchas pelets sin recoger

    # Devolver la evaluación final ajustada por todos los factores
    return score
          \end{lstlisting}
        \subsection*{Implementación final}
          \begin{lstlisting}[language=Python, caption=Implementación final de la función de evaluación]
def betterEvaluationFunction(currentGameState):
    """
    Función de evaluación mejorada para Pacman que incentiva la comida cercana y evita la inactividad.
    """

    # Obtener el estado actual de Pacman
    pacmanPos = currentGameState.getPacmanPosition()
    
    # Obtener la comida restante y convertirla a una lista de posiciones
    food = currentGameState.getFood().asList()
    
    # Obtener las posiciones de los fantasmas y sus estados (asustados o no)
    ghostStates = currentGameState.getGhostStates()
    ghostPositions = [ghost.getPosition() for ghost in ghostStates]
    scaredTimes = [ghostState.scaredTimer for ghostState in ghostStates]
    
    # Obtener las pelets de poder restantes
    capsules = currentGameState.getCapsules()
    
    # Obtener el puntaje actual del juego
    score = currentGameState.getScore()
    
    # Inicializar la evaluación con el puntaje actual
    score = score

    # 1. Incentivar comer comida cercana
    if food:
        minFoodDist = min([manhattanDistance(pacmanPos, foodPos) for foodPos in food])
        # Aumentar el peso de la proximidad a la comida
        score += 10.0 / (minFoodDist + 1)

    # 2. Distancia a los fantasmas: Maximizar la distancia a los fantasmas (si no están asustados)
    for i, ghostPos in enumerate(ghostPositions):
        ghostDist = manhattanDistance(pacmanPos, ghostPos)
        
        if scaredTimes[i] > 0:  # Fantasma asustado
            # Incentivar acercarse a los fantasmas asustados para ganar puntos al comérselos
            score += 20.0 / (ghostDist + 1)
        else:  # Fantasma no asustado
            if ghostDist > 0:
                # Aumentar la penalización por estar cerca de fantasmas no asustados
                score -= 15.0 / ghostDist

    # 3. Comida restante: Penalizar más fuerte la cantidad de comida restante
    score -= 5.0 * len(food)

    # 4. Cápsulas de poder: Incentivar la proximidad a las cápsulas y penalizar su cantidad restante
    if capsules:
        minCapsuleDist = min([manhattanDistance(pacmanPos, capsule) for capsule in capsules])
        score += 8.0 / (minCapsuleDist + 1)
        score -= 200 * len(capsules)  # Penalizar si quedan muchas cápsulas sin recoger
        
    return score
        
          \end{lstlisting}
        \subsection*{Comentarios}
          \paragraph*{}{
            En este caso, ambas implementaciónes son iguales salvo en los pesos asignados a los factores de evaluación. 
            De hecho ambas dan la maxima puntuacion en el autograder.
            En la implementación final se han aumentado los pesos de la proximidad a la comida y a los fantasmas asustados, y se han disminuido los pesos de la cantidad de comida restante y de las cápsulas de poder.
            De esta forma hemos conseguido evitar que Pacman se quede inactivos durante el juego y que se acerque a los fantasmas asustados para ganar puntos.
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|}
                    \hline
                    \textbf{Primera Implementación} & \textbf{Implementación Final} & \textbf{\% de Mejora} \\ \hline
                    1084.0 & 1366.0 & 25.99\% \\ \hline
                    1201.0 & 1183.0 & -1.50\% \\ \hline
                    638.0 & 1338.0 & 109.38\% \\ \hline
                    890.0 & 1204.0 & 35.28\% \\ \hline
                    1096.0 & 1367.0 & 24.72\% \\ \hline
                    1104.0 & 1358.0 & 23.05\% \\ \hline
                    1303.0 & 1377.0 & 5.68\% \\ \hline
                    1175.0 & 1182.0 & 0.60\% \\ \hline
                    1199.0 & 1373.0 & 14.52\% \\ \hline
                    1287.0 & 1342.0 & 4.27\% \\ \hline
                \end{tabular}
                \caption{Comparación de puntuaciones entre la primera implementación y la implementación final}
                \label{tab:comparison}
            \end{table}
            Como se puede apreciar, en la mayoria de casos la implementación final ha mejorado la puntuación respecto a la primera implementación notablemente.  
        }      
    \chapter{Resultados}
      \section{Casos de prueba}
        \subsection{Agente Reflex}
            \begin{lstlisting}[language=Python, caption=Resultados del agente Reflex]
python pacman.py -p ReflexAgent -l testClassic
Pacman emerges victorious! Score: 564
Average Score: 564.0
Scores:        564.0
Win Rate:      1/1 (1.00)
Record:        Win
            \end{lstlisting}
        \clearpage\subsection{Minimax}
            \begin{lstlisting}[language=Python, caption=Resultados del agente Minimax]
python pacman.py -p MinimaxAgent -l trappedClassic -a depth=3
Pacman died! Score: -501
Average Score: -501.0
Scores:        -501.0
Win Rate:      0/1 (0.00)
Record:        Loss
            \end{lstlisting}
        \clearpage\subsection{Podado Alpha-Beta}
            \begin{lstlisting}[language=Python, caption=Resultados del agente Alpha-Beta]
python pacman.py -p AlphaBetaAgent -a depth=3 -l smallClassic
Pacman emerges victorious! Score: 1101
Average Score: 1101.0
Scores:        1101.0
Win Rate:      1/1 (1.00)
Record:        Win
            \end{lstlisting}
        \clearpage\subsection{Expectimax}
            \begin{lstlisting}[language=Python, caption=Resultados del agente Expectimax]
python pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3
Pacman emerges victorious! Score: 511
Average Score: 511.0
Scores:        511.0
Win Rate:      1/1 (1.00)
Record:        Win

python pacman.py -p AlphaBetaAgent -l trappedClassic -a depth=3 -q -n 10
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Pacman died! Score: -501
Average Score: -501.0
Scores:        -501.0, -501.0, -501.0, -501.0, -501.0, -501.0, -501.0, -501.0, -501.0, -501.0
Win Rate:      0/10 (0.00)
Record:        Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss, Loss

python pacman.py -p ExpectimaxAgent -l trappedClassic -a depth=3 -q -n 10
Pacman emerges victorious! Score: 532
Pacman emerges victorious! Score: 532
Pacman died! Score: -502
Pacman died! Score: -502
Pacman emerges victorious! Score: 532
Pacman died! Score: -502
Pacman died! Score: -502
Pacman emerges victorious! Score: 532
Pacman died! Score: -502
Pacman emerges victorious! Score: 532
Average Score: 15.0
Scores:        532.0, 532.0, -502.0, -502.0, 532.0, -502.0, -502.0, 532.0, -502.0, 532.0
Win Rate:      5/10 (0.50)
Record:        Win, Win, Loss, Loss, Win, Loss, Loss, Win, Loss, Win
            \end{lstlisting}
      \clearpage\section{Autograder}
        \begin{lstlisting}[language=Python, caption=Resultados del Autograder]
Starting on 10-26 at 17:50:23

Question q1
===========

Pacman emerges victorious! Score: 1238
Pacman emerges victorious! Score: 1244
Pacman emerges victorious! Score: 1239
Pacman emerges victorious! Score: 1235
Pacman emerges victorious! Score: 1233
Pacman emerges victorious! Score: 1241
Pacman emerges victorious! Score: 1246
Pacman emerges victorious! Score: 1242
Pacman emerges victorious! Score: 1239
Pacman emerges victorious! Score: 1242
Average Score: 1239.9
Scores:        1238.0, 1244.0, 1239.0, 1235.0, 1233.0, 1241.0, 1246.0, 1242.0, 1239.0, 1242.0
Win Rate:      10/10 (1.00)
Record:        Win, Win, Win, Win, Win, Win, Win, Win, Win, Win
*** PASS: test_cases/q1/grade-agent.test (4 of 4 points)
***     1239.9 average score (2 of 2 points)
***         Grading scheme:
***          < 500:  0 points
***         >= 500:  1 points
***         >= 1000:  2 points
***     10 games not timed out (0 of 0 points)
***         Grading scheme:
***          < 10:  fail
***         >= 10:  0 points
***     10 wins (2 of 2 points)
***         Grading scheme:
***          < 1:  fail
***         >= 1:  0 points
***         >= 5:  1 points
***         >= 10:  2 points

### Question q1: 4/4 ###


Question q2
===========

*** PASS: test_cases/q2/0-eval-function-lose-states-1.test
*** PASS: test_cases/q2/0-eval-function-lose-states-2.test
*** PASS: test_cases/q2/0-eval-function-win-states-1.test
*** PASS: test_cases/q2/0-eval-function-win-states-2.test
*** PASS: test_cases/q2/0-lecture-6-tree.test
*** PASS: test_cases/q2/0-small-tree.test
*** PASS: test_cases/q2/1-1-minmax.test
*** PASS: test_cases/q2/1-2-minmax.test
*** PASS: test_cases/q2/1-3-minmax.test
*** PASS: test_cases/q2/1-4-minmax.test
*** PASS: test_cases/q2/1-5-minmax.test
*** PASS: test_cases/q2/1-6-minmax.test
*** PASS: test_cases/q2/1-7-minmax.test
*** PASS: test_cases/q2/1-8-minmax.test
*** PASS: test_cases/q2/2-1a-vary-depth.test
*** PASS: test_cases/q2/2-1b-vary-depth.test
*** PASS: test_cases/q2/2-2a-vary-depth.test
*** PASS: test_cases/q2/2-2b-vary-depth.test
*** PASS: test_cases/q2/2-3a-vary-depth.test
*** PASS: test_cases/q2/2-3b-vary-depth.test
*** PASS: test_cases/q2/2-4a-vary-depth.test
*** PASS: test_cases/q2/2-4b-vary-depth.test
*** PASS: test_cases/q2/2-one-ghost-3level.test
*** PASS: test_cases/q2/3-one-ghost-4level.test
*** PASS: test_cases/q2/4-two-ghosts-3level.test
*** PASS: test_cases/q2/5-two-ghosts-4level.test
*** PASS: test_cases/q2/6-tied-root.test
*** PASS: test_cases/q2/7-1a-check-depth-one-ghost.test
*** PASS: test_cases/q2/7-1b-check-depth-one-ghost.test
*** PASS: test_cases/q2/7-1c-check-depth-one-ghost.test
*** PASS: test_cases/q2/7-2a-check-depth-two-ghosts.test
*** PASS: test_cases/q2/7-2b-check-depth-two-ghosts.test
*** PASS: test_cases/q2/7-2c-check-depth-two-ghosts.test
*** Running MinimaxAgent on smallClassic 1 time(s).
Pacman died! Score: 84
Average Score: 84.0
Scores:        84.0
Win Rate:      0/1 (0.00)
Record:        Loss
*** Finished running MinimaxAgent on smallClassic after 0.7934041023254395 seconds.
*** Won 0 out of 1 games. Average score: 84.0 ***
*** PASS: test_cases/q2/8-pacman-game.test

### Question q2: 5/5 ###


Question q3
===========

*** PASS: test_cases/q3/0-eval-function-lose-states-1.test
*** PASS: test_cases/q3/0-eval-function-lose-states-2.test
*** PASS: test_cases/q3/0-eval-function-win-states-1.test
*** PASS: test_cases/q3/0-eval-function-win-states-2.test
*** PASS: test_cases/q3/0-lecture-6-tree.test
*** PASS: test_cases/q3/0-small-tree.test
*** PASS: test_cases/q3/1-1-minmax.test
*** PASS: test_cases/q3/1-2-minmax.test
*** PASS: test_cases/q3/1-3-minmax.test
*** PASS: test_cases/q3/1-4-minmax.test
*** PASS: test_cases/q3/1-5-minmax.test
*** PASS: test_cases/q3/1-6-minmax.test
*** PASS: test_cases/q3/1-7-minmax.test
*** PASS: test_cases/q3/1-8-minmax.test
*** PASS: test_cases/q3/2-1a-vary-depth.test
*** PASS: test_cases/q3/2-1b-vary-depth.test
*** PASS: test_cases/q3/2-2a-vary-depth.test
*** PASS: test_cases/q3/2-2b-vary-depth.test
*** PASS: test_cases/q3/2-3a-vary-depth.test
*** PASS: test_cases/q3/2-3b-vary-depth.test
*** PASS: test_cases/q3/2-4a-vary-depth.test
*** PASS: test_cases/q3/2-4b-vary-depth.test
*** PASS: test_cases/q3/2-one-ghost-3level.test
*** PASS: test_cases/q3/3-one-ghost-4level.test
*** PASS: test_cases/q3/4-two-ghosts-3level.test
*** PASS: test_cases/q3/5-two-ghosts-4level.test
*** PASS: test_cases/q3/6-tied-root.test
*** PASS: test_cases/q3/7-1a-check-depth-one-ghost.test
*** PASS: test_cases/q3/7-1b-check-depth-one-ghost.test
*** PASS: test_cases/q3/7-1c-check-depth-one-ghost.test
*** PASS: test_cases/q3/7-2a-check-depth-two-ghosts.test
*** PASS: test_cases/q3/7-2b-check-depth-two-ghosts.test
*** PASS: test_cases/q3/7-2c-check-depth-two-ghosts.test
*** Running AlphaBetaAgent on smallClassic 1 time(s).
Pacman died! Score: 84
Average Score: 84.0
Scores:        84.0
Win Rate:      0/1 (0.00)
Record:        Loss
*** Finished running AlphaBetaAgent on smallClassic after 0.6842935085296631 seconds.
*** Won 0 out of 1 games. Average score: 84.0 ***
*** PASS: test_cases/q3/8-pacman-game.test

### Question q3: 5/5 ###


Question q4
===========

*** PASS: test_cases/q4/0-eval-function-lose-states-1.test
*** PASS: test_cases/q4/0-eval-function-lose-states-2.test
*** PASS: test_cases/q4/0-eval-function-win-states-1.test
*** PASS: test_cases/q4/0-eval-function-win-states-2.test
*** PASS: test_cases/q4/0-expectimax1.test
*** PASS: test_cases/q4/1-expectimax2.test
*** PASS: test_cases/q4/2-one-ghost-3level.test
*** PASS: test_cases/q4/3-one-ghost-4level.test
*** PASS: test_cases/q4/4-two-ghosts-3level.test
*** PASS: test_cases/q4/5-two-ghosts-4level.test
*** PASS: test_cases/q4/6-1a-check-depth-one-ghost.test
*** PASS: test_cases/q4/6-1b-check-depth-one-ghost.test
*** PASS: test_cases/q4/6-1c-check-depth-one-ghost.test
*** PASS: test_cases/q4/6-2a-check-depth-two-ghosts.test
*** PASS: test_cases/q4/6-2b-check-depth-two-ghosts.test
*** PASS: test_cases/q4/6-2c-check-depth-two-ghosts.test
*** Running ExpectimaxAgent on smallClassic 1 time(s).
Pacman died! Score: 84
Average Score: 84.0
Scores:        84.0
Win Rate:      0/1 (0.00)
Record:        Loss
*** Finished running ExpectimaxAgent on smallClassic after 1.030266284942627 seconds.
*** Won 0 out of 1 games. Average score: 84.0 ***
*** PASS: test_cases/q4/7-pacman-game.test

### Question q4: 5/5 ###


Question q5
===========

Pacman emerges victorious! Score: 1366
Pacman emerges victorious! Score: 1183
Pacman emerges victorious! Score: 1338
Pacman emerges victorious! Score: 1204
Pacman emerges victorious! Score: 1367
Pacman emerges victorious! Score: 1358
Pacman emerges victorious! Score: 1377
Pacman emerges victorious! Score: 1182
Pacman emerges victorious! Score: 1373
Pacman emerges victorious! Score: 1342
Average Score: 1309.0
Scores:        1366.0, 1183.0, 1338.0, 1204.0, 1367.0, 1358.0, 1377.0, 1182.0, 1373.0, 1342.0
Win Rate:      10/10 (1.00)
Record:        Win, Win, Win, Win, Win, Win, Win, Win, Win, Win
*** PASS: test_cases/q5/grade-agent.test (6 of 6 points)
***     1309.0 average score (2 of 2 points)
***         Grading scheme:
***          < 500:  0 points
***         >= 500:  1 points
***         >= 1000:  2 points
***     10 games not timed out (1 of 1 points)
***         Grading scheme:
***          < 0:  fail
***         >= 0:  0 points
***         >= 10:  1 points
***     10 wins (3 of 3 points)
***         Grading scheme:
***          < 1:  fail
***         >= 1:  1 points
***         >= 5:  2 points
***         >= 10:  3 points

### Question q5: 6/6 ###


Finished at 17:50:33

Provisional grades
==================
Question q1: 4/4
Question q2: 5/5
Question q3: 5/5
Question q4: 5/5
Question q5: 6/6
------------------
Total: 25/25

Your grades are NOT yet registered.  To register your grades, make sure
to follow your instructor's guidelines to receive credit on your project.
        \end{lstlisting}
\end{document}